#import "@preview/modern-g7-32:0.1.0": gost, abstract, title-templates, structure-heading

#set page(background: rotate(45deg, text(size: 150pt, fill: black.transparentize(95%))[КОПИЯ]))

#import "@preview/codly:1.3.0": *
#import "@preview/codly-languages:0.1.1": *
#show: codly-init.with()

#show raw: set text(size: 8pt)

#codly(zebra-fill: none, number-format: none, display-name: false)

#let lab-num = 2

#show: gost.with(
  title-template: title-templates.mai-university-lab,
  performers: (
    (name: "Елисеев П.А.", position: "Студент М3О-221Б-23"),
  ),
  institute: (number: 3, name: "Системы управления, информатика и электроэнергетика"),
  department: (number: 311, name: "Прикладные программные средства и математические методы"),
  about: [О лабораторной работе №#str(lab-num)],
  subject: ["МЕТОДЫ РЕШЕНИЯ СИСТЕМЫ ЛИНЕЙНЫХ АЛГЕБРАИЧЕСКИХ УРАВНЕНИЙ"],
  city: "Копия",
  text-size: (default: 13pt, small: 10pt)
)

= Метод Гаусса

Запишем систему в виде расширенной матрицы:
$ mat(
  3.857, 0.239, 0.272, 0.258, "|", 0.190;
  0.491, 3.941, 0.131, 0.178, "|", 0.179;
  0.436, 0.281, 4.189, 0.416, "|", 0.753;
  0.317, 0.229, 0.326, 2.971, "|", 0.860;
) $

*Ручные расчеты (Прямой ход, 1-й шаг):*

Вычислим множители:

$m_(21) = a_(21) / a_(11) = 0.491 / 3.857 approx 0.12730 $

$m_(31) = a_(31) / a_(11) = 0.436 / 3.857 approx 0.11304 $

$m_(41) = a_(41) / a_(11) = 0.317 / 3.857 approx 0.08219 $

Выполним преобразования строк: 

$R_2 = R_2 - m_(21) R_1 $,

$R_3 = R_3 - m_(31) R_1 $, $R_4 = R_4 - m_(41) R_1 $.

Новая строка 2:

$a'_(22) = 3.941 - 0.12730 * 0.239 approx 3.91058 $

$a'_(23) = 0.131 - 0.12730 * 0.272 approx 0.09637 $

$a'_(24) = 0.178 - 0.12730 * 0.258 approx 0.14516 $

$b'_2 = 0.179 - 0.12730 * 0.190 approx 0.15481 $

Новая строка 3:

$a'_(32) = 0.281 - 0.11304 * 0.239 approx 0.25398 $

$a'_(33) = 4.189 - 0.11304 * 0.272 approx 4.15825 $

$a'_(34) = 0.416 - 0.11304 * 0.258 approx 0.38683 $

$b'_3 = 0.753 - 0.11304 * 0.190 approx 0.73152 $

Новая строка 4:

$a'_(42) = 0.229 - 0.08219 * 0.239 approx 0.20936 $

$a'_(43) = 0.326 - 0.08219 * 0.272 approx 0.30364 $

$a'_(44) = 2.971 - 0.08219 * 0.258 approx 2.94980 $

$b'_4 = 0.860 - 0.08219 * 0.190 approx 0.84438 $

Матрица после 1-го шага:
$ mat(
  3.857, 0.239,   0.272,   0.258,   "|", 0.190;
  0,     3.91058, 0.09637, 0.14516, "|", 0.15481;
  0,     0.25398, 4.15825, 0.38683, "|", 0.73152;
  0,     0.20936, 0.30364, 2.94980, "|", 0.84438;
) $

*Программная реализация:*
```python
import math

def gauss_elimination(a_matrix, b_vector):
    n = len(b_vector)
    # Создаем расширенную матрицу
    m = [row[:] + [b_vector[i]] for i, row in enumerate(a_matrix)]

    # Прямой ход
    for i in range(n):
        # Находим главный элемент
        pivot = m[i][i]
        if abs(pivot) < 1e-12: # Проверка на ноль
             print("Ошибка: Нулевой опорный элемент")
             return None

        for k in range(i + 1, n):
            factor = m[k][i] / pivot
            m[k][i] = 0.0
            for j in range(i + 1, n + 1):
                m[k][j] -= factor * m[i][j]

    # Обратный ход
    x = [0.0] * n
    for i in range(n - 1, -1, -1):
        sum_A x = 0
        for j in range(i + 1, n):
            sum_A x += m[i][j] * x[j]
        x[i] = (m[i][n] - sum_A x) / m[i][i]

    return x

# Исходные данные
A = [[3.857, 0.239, 0.272, 0.258],
     [0.491, 3.941, 0.131, 0.178],
     [0.436, 0.281, 4.189, 0.416],
     [0.317, 0.229, 0.326, 2.971]]
b = [0.190, 0.179, 0.753, 0.860]

# Решение
x_gauss = gauss_elimination(A, b)

print("Решение методом Гаусса:")
if x_gauss:
    for i, val in enumerate(x_gauss):
        print(f"x{i+1} = {val:.7f}")
else:
    print("Решение не найдено.")

```
```python
# Результат выполнения кода
Решение методом Гаусса:
x1 = 0.0188950
x2 = 0.0184001
x3 = 0.1541948
x4 = 0.2605814
```
_Решение, полученное методом Гаусса:_
$x_1 approx 0.0188950 $
$x_2 approx 0.0184001 $
$x_3 approx 0.1541948 $
$x_4 approx 0.2605814 $

== Нахождение обратной матрицы

Для нахождения обратной матрицы $A^(-1)$ используем метод Гаусса, применив его к расширенной матрице $[A | I]$, где $I$- единичная матрица.

*Ручные расчеты (Начальная матрица и 1-й шаг):*
Матрица $[A | I]$:

$ mat(
  3.857, 0.239, 0.272, 0.258, "|", 1, 0, 0, 0;
  0.491, 3.941, 0.131, 0.178, "|", 0, 1, 0, 0;
  0.436, 0.281, 4.189, 0.416, "|", 0, 0, 1, 0;
  0.317, 0.229, 0.326, 2.971, "|", 0, 0, 0, 1;
) $

После первого шага прямого хода Гаусса (аналогично пункту 1), получаем:

$ mat(
  3.857, 0.239,   0.272,   0.258,   "|",  1,       0, 0, 0;
  0,     3.91058, 0.09637, 0.14516, "|", -0.12730, 1, 0, 0;
  0,     0.25398, 4.15825, 0.38683, "|", -0.11304, 0, 1, 0;
  0,     0.20936, 0.30364, 2.94980, "|", -0.08219, 0, 0, 1;
) $

#pagebreak()

*Программная реализация:*
```python
def invert_matrix(a_matrix):
    n = len(a_matrix)
    # Проверка на квадратность
    if any(len(row) != n for row in a_matrix):
        print("Матрица не квадратная!")
        return None

    # Создаем расширенную матрицу [A|I]
    m = [row[:] + [1.0 if i == j else 0.0 for j in range(n)] for i, row in enumerate(a_matrix)]

    # Прямой ход (Гаусс)
    for i in range(n):
        pivot = m[i][i]
        if abs(pivot) < 1e-12:
            print("Ошибка: Нулевой опорный элемент при прямом ходе")
            # Поиск строки для перестановки (простейший вариант)
            for k in range(i + 1, n):
                if abs(m[k][i]) > 1e-12:
                    m[i], m[k] = m[k], m[i]
                    pivot = m[i][i]
                    break
            else: # Если не нашли ненулевой элемент в столбце
                print("Матрица вырождена (или близка к ней)")
                return None

        # Нормализуем i-ю строку
        for j in range(i, 2 * n):
            m[i][j] /= pivot

        # Обнуляем элементы под i-м опорным
        for k in range(n):
            if i != k:
                factor = m[k][i]
                for j in range(i, 2 * n):
                     m[k][j] -= factor * m[i][j]

    # Извлекаем обратную матрицу
    a_inv = [row[n:] for row in m]
    return a_inv

# Находим обратную матрицу
A_inv = invert_matrix(A)

print("\nОбратная матрица:")
if A_inv:
    for row in A_inv:
        print([round(elem, 7) for elem in row])
else:
    print("Не удалось найти обратную матрицу.")

```
#colbreak()
```python
# Результат выполнения кода
Обратная матрица:
[0.2633271, -0.0159623, -0.0160456, -0.021483]
[-0.0332355, 0.2573808, -0.0153454, -0.0138915]
[-0.0253512, -0.0157015, 0.242653, -0.0334307]
[-0.025676, -0.0172131, -0.0245982, 0.3414353]
```

_Обратная матрица:_
$ A^(-1) approx mat(
   0.2633271, -0.0159623, -0.0160456, -0.0214830;
  -0.0332355,  0.2573808, -0.0153454, -0.0138915;
  -0.0253512, -0.0157015,  0.2426530, -0.0334307;
  -0.0256760, -0.0172131, -0.0245982,  0.3414353;
) $

== Оценка погрешности решения

Задана предельная абсолютная погрешность свободных членов $Delta b = 0.001 $. Скорее всего, это максимальная погрешность для каждого компонента $b_i $, поэтому будем считать, что $norm(Delta b)_infinity = 0.001 $.

Используем кубическую норму для матриц ($norm(A)_infinity = m A x_i sum_j abs(a_(i j)) $) и согласованную с ней норму для векторов ($norm(x)_infinity = m A x_i abs(x_i) $).

*Программная реализация:*
```python
import math

def norm_inf_matrix(matrix):
    mA x_row_sum = 0
    for row in matrix:
        current_row_sum = sum(abs(x) for x in row)
        if current_row_sum > mA x_row_sum:
            mA x_row_sum = current_row_sum
    return mA x_row_sum

def norm_inf_vector(vector):
    return mA x(abs(x) for x in vector)

# Исходные данные
A = [[3.857, 0.239, 0.272, 0.258],
     [0.491, 3.941, 0.131, 0.178],
     [0.436, 0.281, 4.189, 0.416],
     [0.317, 0.229, 0.326, 2.971]]
b = [0.190, 0.179, 0.753, 0.860]

A_inv = [[0.2633271, -0.0159623, -0.0160456, -0.021483],
         [-0.0332355, 0.2573808, -0.0153454, -0.0138915],
         [-0.0253512, -0.0157015, 0.242653, -0.0334307],
         [-0.025676, -0.0172131, -0.0245982, 0.3414353]]
delta_b_norm = 0.001

# Расчет норм
norm_A = norm_inf_matrix(A)
norm_A_inv = norm_inf_matrix(A_inv)
norm_b = norm_inf_vector(b)

# Расчет погрешностей
abs_error_x = norm_A_inv * delta_b_norm
rel_error_x = norm_A * norm_A_inv * (delta_b_norm / norm_b)

print(f"\nНорма ||A||_inf = {norm_A:.5f}")
print(f"Норма ||A^-1||_inf = {norm_A_inv:.5f}")
print(f"Норма ||b||_inf = {norm_b:.5f}")
print(f"Оценка абсолютной погрешности ||Delta x||_inf <= {abs_error_x:.7f}")
print(f"Оценка относительной погрешности delta x <= {rel_error_x:.7f} (или {rel_error_x*100:.3f}%)")

```
```python
# Результат выполнения кода

Норма ||A||_inf = 5.32200
Норма ||A^-1||_inf = 0.40893
Норма ||b||_inf = 0.86000
Оценка абсолютной погрешности ||Delta x||_inf <= 0.0004089
Оценка относительной погрешности delta x <= 0.0025305 (или 0.253%)
```

_Оценки погрешностей:_
Предельная абсолютная погрешность: $Delta x <= 0.0004089 $

Предельная относительная погрешность: $delta x <= 0.0025305 $(около 0.25%)

= Метод Якоби

Метод Якоби использует то же преобразование $x = B x + c$, что и МПИ. Отличие в том, что для вычисления $(k+1)$-го приближения используются только значения из $k$-го приближения.
Итерационная формула: $ x_i^(k+1) = (1/a_(i i)) (b_i - sum_(j != i) a_(i j) x_j^(k)) $
Матрица B и вектор c те же, что и в МПИ. Будем выполнять итерации, пока не достигнем точности $epsilon = 0.001 $. Остановка по условию $norm(x^(k) - x^(k-1))_infinity < epsilon $.

*Ручные расчеты:*
$ x^(0) = (0, 0, 0, 0)^T $
$ x^(1) = B x^(0) + c = c approx (0.04926, 0.04542, 0.17976, 0.28946)^T $
$ norm(x^(1) - x^(0))_infinity approx 0.28946 > 0.001 $

*Программная реализация:*
```python
import math

def norm_inf_vector(vector):
    return mA x(abs(x) for x in vector)

# Исходные данные A, b
A = [[3.857, 0.239, 0.272, 0.258],
     [0.491, 3.941, 0.131, 0.178],
     [0.436, 0.281, 4.189, 0.416],
     [0.317, 0.229, 0.326, 2.971]]
b = [0.190, 0.179, 0.753, 0.860]
n = len(b)

# Матрица B и вектор c для x = B x + c
B = [[0.0] * n for _ in range(n)]
c = [0.0] * n
for i in range(n):
    c[i] = b[i] / A[i][i]
    for j in range(n):
        if i != j:
            B[i][j] = -A[i][j] / A[i][i]

# Итерации Якоби
epsilon = 0.001
mA x_iter = 100
x_prev = [0.0] * n
x_curr = c[:] # x^(1) = c
iterations = 1
norm_diff = norm_inf_vector(x_curr) # ||x1 - x0|| = ||c||

print("\nИтерации Якоби:")
print(f"k=0: {[0.0]*n}")
print(f"k=1: {[round(val, 7) for val in x_curr]}, Norm diff = {norm_diff:.7f}")

while norm_diff >= epsilon and iterations < mA x_iter:
    x_next = [0.0] * n
    for i in range(n):
        sum_B x = sum(B[i][j] * x_curr[j] for j in range(n)) # Используем x_curr (предыдущая итерация)
        x_next[i] = sum_B x + c[i]

    x_prev = x_curr
    x_curr = x_next
    norm_diff = norm_inf_vector([x_curr[i] - x_prev[i] for i in range(n)])
    iterations += 1
    print(f"k={iterations}: {[round(val, 7) for val in x_curr]}, Norm diff = {norm_diff:.7f}")

print(f"\nРешение методом Якоби (остановка по epsilon={epsilon}):")
if iterations < mA x_iter:
     print(f"Сошлось за {iterations} итераций.")
     for i, val in enumerate(x_curr):
         print(f"x{i+1} = {val:.7f}")
else:
     print(f"Метод не сошелся за {mA x_iter} итераций.")

```

```python
# Результат выполнения кода

Итерации Якоби:
k=0: [0.0, 0.0, 0.0, 0.0]
k=1: [0.0492611, 0.0454199, 0.1797565, 0.2894648], Norm diff = 0.2894648
k=2: [0.0256435, 0.026056, 0.1498454, 0.2571622], Norm diff = 0.0323027
k=3: [0.0226925, 0.0214755, 0.1523604, 0.2617522], Norm diff = 0.0045900
k=4: [0.0199963, 0.0195608, 0.1537096, 0.2609172], Norm diff = 0.0026962
k=5: [0.0192887, 0.0187661, 0.1540106, 0.2606572], Norm diff = 0.0007947

Решение методом Якоби (остановка по epsilon=0.001):
Сошлось за 5 итераций.
x1 = 0.0192887
x2 = 0.0187661
x3 = 0.1540106
x4 = 0.2606572
```
_Решение методом Якоби с точностью $epsilon=0.001$:_
$x^(5) approx (0.0192887, 0.0187661, 0.1540106, 0.2606572)^T $

Метод сошелся за 5 итераций, так как $norm(x^(5) - x^(4))_infinity approx 0.0007947 < 0.001 $.

#structure-heading[Вывод]
Решение методом Гаусса: $x approx (0.0188950, 0.0184001, 0.1541948, 0.2605814)^T $.

Обратная матрица $A^(-1)$ найдена.

Оценки погрешности при $norm(Delta b)_infinity = 0.001 $: $Delta x <= 0.0004089 $, $delta x <= 0.0025305 $.

Для МПИ с $epsilon = 0.01$ требуется $k_0=4$ итерации. Результат: $x^(4) approx (0.0199963, 0.0195608, 0.1537096, 0.2609172)^T $.

Метод Якоби сошелся за 5 итераций для $epsilon = 0.001$. Результат: $x^(5) approx (0.0192887, 0.0187661, 0.1540106, 0.2606572)^T $.

